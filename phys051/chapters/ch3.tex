\documentclass[../p051main.tex]{subfiles}
\graphicspath{{\subfix{../figures/}}}

\begin{document}

\chapter{Electrodynamics}
\section{Faraday's Law}
Now it's time to bring time-dependence into the picture by allowing electric and magnetic fields to change over time.
Before we begin, though, we should take a closer look at the actual phenomenon responsible for current in wires.

There are two things that contribute to driving current around a loop of wire: a source $\mbf{f}$ that does the pushing and an electrostatic field that smooths the current flow throughout the wire.
The source can take a variety of forms---we often think of it as a chemical force confined to the inside of a battery---but whatever it is, its net effect is characterized by the electromotive force (or emf),
\[ \mathcal{E} = \oint \mbf{f} \cdot d\mbf{l}. \]
Note that, confusingly, $\mathcal{E}$ is not a force but rather the integral of a force per unit charge.
This means emf has units of potential, and we can often interpret it as such, like in the case of an ideal battery.
The following example illustrates a case in which this is not so.

\begin{example}[Motional emf]
    Consider a height-$h$ rectangular loop of wire whose left half is enveloped by a uniform magnetic field $\mbf{B}$ pointing into the page.
    Suppose we pull the loop toward the right with velocity $\mbf{v}$; an upward magnetic force acts on the charges in the ``vertical wire segment'', generating a clockwise current.

    The magnetic force has done no work here (the extra energy comes from our pull), it still creates an emf
    \[ \mathcal{E} = \oint_\textrm{wire} \hspace{-5pt} (\mbf{v} \times \mbf{B}) \cdot d\mbf{l} = vBh. \]
\end{example}

This simple example is indicative of a more general rule.
Whenever the magnetic flux $\Phi$ through a loop of wire changes, we have
\[ \mathcal{E} = -\frac{d\Phi}{dt} = -\frac{d}{dt} \iint \mbf{B} \cdot d\mbf{A}. \]
It turns out that this is our new, time-dependent version of Faraday's law.
We'll come back to this in a moment.
Now, although the above equation leaves no ambiguity regarding the direction of the induced emf (it is determined by the right-hand rule), the signs can get confusing.
To get the directions right, we introduce Lenz's law: the induced current will flow in such a way that opposes the change in magnetic flux.
If, for example, a magnetic field into the page is increasing in magnitude, then current will flow counterclockwise to create a magnetic field out of the page.

The above form of Faraday's law makes sense if the emf is induced by the wire's motion through space, but experiment shows that it also holds for stationary wires in changing magnetic fields.
In this case the emf cannot be magnetic in nature, so it must be electric.
Specifically, a changing magnetic field $\mbf{B}$ induces an electric field $\mbf{E}$ that satisfies
\[ \oint_{\partial S} \mbf{E} \cdot d\mbf{l} = -\frac{d}{dt} \iint_S \mbf{B} \cdot d\mbf{A}. \]
This is the modified version of Faraday's law that we'll include in Maxwell's equations.
We could use Stokes's theorem to get the differential form,
\[ \nabla \times \mbf{E} = -\frac{\partial \mbf{B}}{\partial t}. \]

\section{Inductors}
One surprising consequence of Faraday's law is that it allows a circuit to induce an emf in itself.
Consider an open, clockwise-oriented circuit that is isolated from any external magnetic fields; when the circuit is closed, current flows around the wire and generates a magnetic flux into the page.
But this generation isn't instantaneous, meaning there is a period where $d\Phi / dt$ is nonzero and, consequentially, where there is an emf opposing the change in current.

This phenomenon is called self-inductance, and the opposing emf is called back-emf.
The magnitude of the back-emf is determined by the inductance $L$ of the circuit, defined by
\[ \mathcal{E} = -L \frac{di}{dt}. \]
Inductance is measured in henries (H) and depends on the relevant circuit's material and geometric properties.

\begin{example}[Inductance of a solenoid]
    Consider a radius-$r$ solenoid with $n$ turns per unit length, spanning a length $l$.
    A current $i$ flows through the solenoid.
    The resulting magnetic flux is given by
    \[ \Phi = \iint_\textrm{sol} \mbf{B} \cdot d\mbf{A} = nl \iint_\textrm{loop} \hspace{-5pt} \mbf{B} \cdot d\mbf{A} = nl \cdot \mu_0 ni \cdot \pi r^2. \]
    By Faraday's law, the induced emf produced by a changing current has magnitude
    \[ |\mathcal{E}| = \left| \frac{d}{dt} \left( nl \cdot \mu_0 ni \cdot \pi r^2 \right) \right| = \mu_0 n^2 (\pi r_s^2 l) \cdot \left| \frac{di}{dt} \right|. \]
    Thus the inductance of the solenoid is
    \[ L_\textrm{sol} = \mu_0 n^2 \left( \pi r_s^2 l \right), \]
    where $A$ is the area of each loop.
\end{example}

Solenoid-like objects often make appearances in electrical circuits as inductors---they serve to increase the effective area enclosed by the circuit, generating a stronger back-emf in responses to changes in current.
Since the area created by an inductor is usually much greater than the other area enclosed by the circuit, for our purposes we can model the entire circuit's inductance with a single inductor.

Like every other circuit element, inductors have potential differences across their terminals.
It is simply given by $\mathcal{E}$---when the current is increasing there is a potential drop, and when current is decreasing there is a rise.
Like we have in the past, we can use this with conservation of energy to construct a differential equation that describes the current dynamics in a circuit.

Inductors store energy in the magnetic field in a fashion analogous to how capacitors store energy in the electric field.
The flow of energy into an inductor is given by
\[ P = i\mathcal{E}_\textrm{ind} = iL \frac{di}{dt}, \]
so the overall work done in charging the conductor is
\[ W = \int P \,dt = \frac{1}{2} Li^2. \]
This is equivalent to the amount of "magnetic" potential energy $U_B$ stored in the inductor.
We can also define an energy density by determining the energy per unit volume stored in a solenoid:
\[ u_B = \frac{\frac{1}{2} L_\textrm{sol} i^2}{\pi r_\textrm{sol}^2 l_\textrm{sol}} = \frac{1}{2\mu_0}B^2. \]
This gives us another way to determine the inductance of an object---given a magnetic field and a current, we can integrate an energy density over the object's volume to find its stored energy and solve for its inductance.

\section{Maxwell's Equations}
We've found that changing magnetic fields generate electric fields.
But in order to preserve the relativistic symmetry between the two fields, we should also expect that changing electric fields generate magnetic fields.
We'll use a thought experiment to resolve this discrepancy!

Suppose a current $i$ charges a standard parallel-plate capacitor.
We'll draw an Amperian circle around the current, accompanied by two different surfaces $S_1$ and $S_2$ with that circle as their mutual boundary.
\begin{itemize}
    \item $S_1$ is simply the filled-in disk corresponding to the Amperian circle.
    It encloses a current $i$, so by Ampere's law we have $B = \mu_0 i / 2\pi r$, where $r$ is the radius of the Amperian circle.

    \item $S_2$ is an open cylinder whose cap is between the plates of the capacitor.
    Since this surface encloses no current, our version of Ampere's law gives $B = 0$.
\end{itemize}
These two cases are clearly at odds with each other---Ampere's law should give the same result no matter what surface we use.
We might propose that the capacitor's time-varying electric flux can solve the problem.
This would give an equation of the form
\[ \oint_{\partial S} \mbf{B} \cdot d\mbf{l} = \mu_0 i_\textrm{enc} \,+\; ? \frac{d}{dt} \iint_S \mbf{E} \cdot d\mbf{A}, \]
where $?$ represents a constant that we'll now solve for.
Noting that the electric field between the capacitor plates is $E = q / (A \epsilon_0)$, where $A$ is the area of each plate, using $S_2$ as our surface we get from the above
\begin{align*}
    B(r) \cdot 2 \pi r &= \;?\, \frac{d}{dt} \iint_\textrm{cap} \mathbf{E} \cdot d\mathbf{A} = \;?\, \frac{d}{dt} \frac{q}{A \varepsilon_0} A = \frac{?}{\varepsilon_0} \frac{dq}{dt}. \\
    B(r) &= \frac{?\, i}{\varepsilon_0 \cdot 2 \pi r}
\end{align*}
So $B(r) = (? i) / (2\pi r \epsilon_0)$, and in order to be consistent with our result for $S_1$ we must have $? = \mu_0 \epsilon_0$.
We've resolved the discrepancy!
From here we could again use Stokes's theorem to derive the differential form.
Note that Ampere's law can be written as
\[ \oint_{\partial S} \mbf{B} \cdot d\mbf{l} = \mu_0 \left( i_\textrm{enc} \,+\; \epsilon_0 \frac{d}{dt} \iint_S \mbf{E} \cdot d\mbf{A} \right), \]
so the latter term in the parentheses has units of current.
Confusingly, this term is called the displacement current despite having nothing to do with actual current at all.
Another example of vestigial jargon.

Anyway, we've finally arrived at the complete set of Maxwell's equations, the fundamental principles of classical electromagnetism.
\begin{align*}
    \oiint_S \mathbf{E} \cdot d\mathbf{A} &= \frac{q_\text{enc}}{\varepsilon_0} & \nabla \cdot \mbf{E} &= \frac{\rho}{\epsilon_0} \\
    \oint_{\partial S} \mathbf{E} \cdot d\mathbf{l} &= -\frac{d}{dt} \iint_S \mathbf{B} \cdot d\mathbf{A} & \nabla \times \mbf{E} &= -\frac{\partial \mbf{B}}{\partial t} \\
    \oiint_S \mathbf{B} \cdot d\mathbf{A} &= 0 & \nabla \cdot \mbf{B} &= \mbf{0} \\
    \oint_{\partial S} \mathbf{B} \cdot d\mathbf{l} &= \mu_0 i_\text{enc} + \mu_0 \varepsilon_0 \frac{d}{dt}\iint_S \mathbf{E} \cdot d\mathbf{A} & \nabla \times \mbf{B} &= \mu_0 \mbf{j} + \mu_0 \epsilon_0 \frac{\partial \mbf{E}}{\partial t}
\end{align*}
Everything we know about electricity and magnetism is embedded in these laws.
For example, we can take the divergence of Ampere's law to get charge conservation:
\begin{align*}
    \nabla \cdot \nabla \times \mathbf{B} &= \nabla \cdot \left( \mu_0 \mathbf{j} + \mu_0 \varepsilon_0 \frac{\partial \mathbf{E}}{\partial t} \right) \\
    0 &= \nabla \cdot \mu_0 \mathbf{j} +  \nabla \cdot \mu_0 \varepsilon_0 \frac{\partial \mathbf{E}}{\partial t} \\
    0 &= \nabla \cdot \mathbf{j} + \frac{\partial \rho}{\partial t}
\end{align*}
In words, the current density flowing into or out of a point is equal and opposite the change in charge density.

\section{Electromagnetic Waves}
Let's see what happens when we take the curl of Faraday's law.
Applying a vector calculus identity gives
\begin{align*}
    \nabla \times \nabla \times \mbf{E} &= -\frac{\partial}{\partial t} (\nabla \times \mbf{B}) \\
    \nabla (\nabla \cdot \mbf{E}) - \nabla^2 \mbf{E} &= -\frac{\partial}{\partial t} \left( \mu_0 \epsilon_0 \frac{\partial \mbf{E}}{\partial t} \right) \\
    \nabla^2 \mbf{E} &= \frac{1}{c^2} \frac{\partial^2 \mbf{E}}{\partial t^2}
\end{align*}
We recognize this as the wave equation in $\mbf{E}$!
(A similar line of reasoning for magnetic fields gives the wave equation in $\mbf{B}$.)
Recall that solutions to this equation take the form $\mbf{E}(x,y,z,t) = \mbf{E}_0 f(x \pm ct)$, which corresponds to a wave propagating in the $\mp x$-direction with speed $c$.
This is what we call light!

These electromagnetic waves come with some nice properties.
If we have an electric field $\mbf{E} = \mbf{E}_0 f(x - ct)$, then
\[ \nabla \cdot \mbf{E} = E_{0x} \frac{\partial}{\partial x} f(x - ct) = E_{0x} f'(x - ct). \]
By Gauss's law this must evaluate to zero.
It doesn't really make sense for a wave in motion to have $f'(x - ct) = 0$, so we must have $E_{0x}$.
Thus $\mbf{E}$ cannot have a component along the direction of wave propagation!
A similar analysis would show that $\mbf{B}$ cannot have such a component, either.

Let's take a closer look at the relationship between the two fields.
If we take $\mbf{E}$ to point in the $\hat y$ direction, then we'd get a curl
\[ \nabla \times \mbf{E} = \hat z \,\frac{\partial}{\partial z} E_0 f(x - ct); \]
by Faraday's law this is equivalent to $-\partial \mbf{B} / \partial t$, meaning the electric and magnetic fields are orthogonal to one another!
Specifically, if we have $\mbf{B} = B_0 \hat z \,f(x - ct)$, then taking a time derivative yields
\[ \frac{\partial \mbf{B}}{\partial t} = -\hat z \,cB_0 f'(x - ct), \]
just as we got above.
Since $\hat y \times \hat z = \hat x$, this means $\mbf{E} \times \mbf{B}$ points in the electromagnetic wave's direction of propagation.
We also get a relationship between the amplitudes of the fields,
\[ B_0 = \frac{E_0}{c}. \]
If we wanted, we could verify that all this theory has remained consistent with Maxwell's equations.
To verify Faraday's integral law we might take a tall, thin loop in the plane of $\mbf{E}$ and qualitatively argue that the sign of the line integral about the loop is opposite that of the corresponding change in magnetic flux.
For Ampere's law we'd do something similar with a loop in the plane of $\mbf{B}$.

\section{The Poynting Vector}
Aided by a general understanding of electromagnetic waves, we're ready to take a more detailed look at how energy fits into the broad picture of electrodynamics.
We'll begin with the simple observation that the total energy stored by the electromagnetic field is given by
\[ U_T = \iiint(u_E + u_B) \,d(\textrm{vol.}) = U_E + U_B = \frac{1}{2} \left( \epsilon_0 E^2 + \frac{1}{\mu_0} B^2 \right). \]
In the special case of an electromagnetic wave we have $B^2 = \mu_0 \epsilon_0 E^2$, which gives $U_T = \epsilon_0 E^2$.
So in such a wave, the field energy is equally electric and magnetic!

Now we'll look at the dynamics of this total field energy.
We'll first dot the electric field onto both sides of Ampere's law and apply a couple of vector identities:
\begin{align*}
    \mathbf{E} \cdot (\nabla \times \mathbf{B}) &= \mu_0 \varepsilon_0 \,\mathbf{E} \cdot \frac{\partial \mathbf{E}}{\partial t}, \\
    \mathbf{B} \cdot (\nabla \times \mathbf{E}) - \nabla \cdot (\mathbf{E} \times \mathbf{B}) &= \frac{\mu_0 \varepsilon_0}{2} \frac{\partial}{\partial t} E^2. \\
    \intertext{By Faraday's law,}
    -\nabla \cdot (\mathbf{E} \times \mathbf{B}) &= \frac{1}{2} \mu_0 \varepsilon_0 \frac{\partial}{\partial t} E^2 + \mathbf{B} \cdot \left( \frac{\partial}{\partial t} \mathbf{B} \right), \\
    -\nabla \cdot \left( \frac{\mathbf{E} \times \mathbf{B}}{\mu_0} \right) &= \frac{1}{2} \varepsilon_0 \frac{\partial}{\partial t} E^2 + \frac{1}{2\mu_0} \frac{\partial}{\partial t} B^2. \\
    \intertext{We might recognize this as}
    -\nabla \cdot \left( \frac{\mathbf{E} \times \mathbf{B}}{\mu_0} \right) &= \frac{\partial}{\partial t} u_T, \\
    \intertext{which we can integrate to get}
    \oiint_{\partial V} \left( \frac{\mathbf{E} \times \mathbf{B}}{\mu_0} \right) \cdot (-d\mathbf{A}) &= \frac{d}{dt} \iiint_V u_T \;d(\textrm{vol.}).
\end{align*}
If we look closely, we might see that this is a statement about conservation of energy!
The right side is the rate at which the ``electromagnetic energy'' inside a volume $V$ changes, so we can infer that the left side describes the inward flux of energy through the boundary $\partial V$.
Thus the integrand
\[ \mbf{S}_P = \frac{\mbf{E} \times \mbf{B}}{\mu_0}, \]
called the Poynting vector, describes the energy transported by the electromagnetic field per unit time, per unit area.
(We might call $\mbf{S}$ an energy flux density.)
This relationship is fundamental to the study of energy in electrodynamics, so naturally we can use it to do some pretty cool things!
For example, we could use it to prove that the power emitted by a resistor is given by $i^2 R$.

Electromagnetic waves, of course, carry energy, so the Poynting vector naturally coincides with an such a wave's direction of propagation.
But while we usually think of a light beam as having a fixed energy, in reality the magnitude of $\mbf{S}_P$ oscillates rapidly between extrema.
So we define the intensity $I$ of an electromagnetic wave as the magnitude of its time-averaged Poynting vector; since $\left< \sin^2 (x - ct) \right> = 1 / 2$, we have
\[ I = \left< S_P \right> = \left< \frac{EB}{\mu_0} \right> = \left< \frac{E_0^2 \sin^2(x - ct)}{\mu_0c} \right> = \frac{1}{2\mu_0} \frac{E_0^2}{c} = \frac{1}{2} \epsilon_0 cE_0^2. \]
More surprisingly, we know from special relativity that light carries a momentum $p = E / c$.
We can use this to define radiation pressure, the force $F$ an electromagnetic wave exerts on a surface per unit area $A$.
By Newton's second law,
\[ \textrm{pressure} = \frac{1}{A} \frac{dp}{dt} = \frac{1}{cA} \frac{dE}{dt} = \begin{cases} S / c \textrm{ (absorber)} \\ 2S / c \textrm{ (reflector)} \end{cases}. \]
The separate cases here arise from conservation of momentum.
Absorbers simply take on the momentum of any incident light, while reflectors must double that amount in order to turn the light around.

\section{Magnetism in Materials}
We'll cap off our study of electrodynamics by looking at how magnetic fields interact with matter.
So far we've studied these fields in the context of macroscopic charge movements---through a wire, for example---but we know from real-world experience that magnets also arise when there is no obvious movement of charge.
In cases like these, we must zoom in to the microscopic level.

Electrons are known to have spin angular momentum.
For our purposes, we may model this by having each electron spin about a central axis, generating a little loop of current $i$ and thus a dipole moment $\boldsymbol{\mu} = i\mbf{A}$.
Electrons that are ``paired'' with one another cancel each others' dipole moments, but unpaired electrons are left to act as permanent dipole moments.

When a collection of these permanent dipoles is isolated from external magnetic fields, they all point in random directions that tend to cancel each other out, so no magnetism arises.
But under the influence of a uniform field $\mbf{B}$, there is a net torque that tries to orient each loop such that $\boldsymbol{\mu}$ coincides with the direction of $\mbf{B}$.
We could show (quite easily for rectangles) that the torque on each loop is given by $\boldsymbol{\tau} = \boldsymbol{\mu} \times \mbf{B}$, which gives the configuration energy $U_B = -\boldsymbol{\mu} \cdot \mbf{B}$ if the loop does not move.
In practice the loop does move, and all of the dipoles become aligned with the external magnetic field in a phenomenon called paramagnetism.

Now, electrons are known to also have orbital angular momentum, which we'll model as literal orbits generating slightly larger loops of current.
Paired electrons are modeled by two loops of wire with equal opposite currents, producing no net magnetic dipole in isolation.
When influenced by an external magnetic field, by Lenz's law the loops of wire together create a magnetic field that opposes the induced field; in aggregate, this phenomenon is called diamagnetism.

Magnetic fields are reduced in diamagnetic materials.
To quantify this reduction, we can define a ``diamagnetic constant'' $\kappa_m$ in a similar fashion to the dielectric constant: if $\mbf{B}_0$ is the applied field and $\mbf{B}$ is the resultant field, then $\mbf{B} = \kappa_m \mbf{B}_0$.
For diamagnetic materials we have $\kappa_m < 1$, while for paramagnetic materials $\kappa_m > 1$.
(Note that all materials are at least somewhat diamagnetic, but its effect is often very weak.)

Finally, when we take a paramagnet and allow the permanent dipoles to interact with one another in such a way that they remain aligned after removing the external magnetic field, we get a ferromagnet.
These materials have $\kappa_m \gg 1$.
Since interactions between atoms tend to decrease with temperature, each ferromagnet has a Curie temperature above which they become paramagnetic.

\end{document}