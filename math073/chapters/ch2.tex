\documentclass[../m073main.tex]{subfiles}
\graphicspath{{\subfix{../figures/}}}

\begin{document}

\chapter{Systems of Linear Equations}
\section{Systems of Linear Equations}
We now turn our attention to systems of simultaneous linear equations.
(Vectors will make a return later.)
We must first define what, exactly, a linear equation is, and what it means to solve one.

\begin{definition}[Linear equation]
	A linear equation in the $n$ variables $x_1, x_2, \ldots, x_n$ is an equation that can be written in the form
	\[ a_1 x_1 + a_2 x_2 + \cdots + a_n x_n = b \]
	where the coefficients $a_1, a_2, \ldots, a_n$ and the constant term $b$ are constants.
	A solution of this equation is a vector $\begin{bmatrix} s_1 & s_2 & \cdots & s_n \end{bmatrix}$ whose components satisfy the equation when we substitute $x_i = s_i$ for all $i = 1, 2, \ldots, n$.
\end{definition}

Now that we've defined linear equations, we can define what a system of linear equations is.

\begin{definition}[System of linear equations]
	A system of linear equations (or a linear system) is a finite set of linear equations, each with the same variables.
	A solution of a linear system is a vector that is simultaneously a solution of each equation in the system; the solution set of a linear system is the set of all solutions of the system.
\end{definition}

Finally, we establish some terminology regarding the solutions to a linear system.

\begin{definition}[Consistent system]
	A linear system is consistent if it has at least one solution.
	A system with no solutions is inconsistent.
\end{definition}

\section{Solving Linear Systems}
Our general technique for solving systems of linear equations will be to gradually simplify the system to the point where the solution is easy to find.
We first give a condition for this simplification.

\begin{definition}[Equivalent systems]
	Two linear systems are equivalent if they have the same solution set.
\end{definition}

Now, when we're working with a linear system, all that really matters are the coefficients and constants of the linear system.
Extracting these numbers and arranging them in a grid makes solution a bit less cumbersome.
It also allows us to analyze linear systems in a more sophisticated way, which we'll see later.

\begin{definition}[Matrix of a linear system]
	A linear system's coefficient matrix contains the coefficients of the variables.
	A system's augmented matrix consists of its coefficient matrix, along with an extra column containing the system's constant terms.
\end{definition}

There's a couple of ways we can define the ``simplest'' form of a system's matrix, coefficient or augmented.
The first of these is given below.

\begin{definition}[Row echelon form]
	A matrix is in row echelon form (REF) if it satisfies the following properties.
	\begin{enumerate}
		\item Any rows consisting entirely of zeros are at the bottom.
		\item In each nonzero row, the first nonzero entry (called the leading entry) is in a column to the left of any leading entries below it.
	\end{enumerate}
\end{definition}

Based on how each row of a system's matrix represents one of the equations in the system, we get a few rules governing how we can convert a matrix into a row echelon form.

\begin{definition}[Elementary row operations]
	The following elementary row operations (EROs) can be performed on a matrix.
	\begin{enumerate}
		\item Interchange two rows.
		\item Multiply a row by a nonzero constant.
		\item Add a multiple of a row to another row.
	\end{enumerate}
	The process of applying elementary row operations to bring a matrix into REF is called row reduction.
\end{definition}

These rules help us reason what it means for two matrices to represent equivalent systems.

\begin{definition}[Row equivalent matrices]
	Two matrices are row equivalent if there is a series of EROs that converts one into the other.
\end{definition}

We don't need to find such a series every time we want to determine whether two matrices are row equivalent.
Rather, we can use the following condition.

\begin{theorem}[Condition for row equivalence]
	Matrices $A$ and $B$ are row equivalent if and only if they can be reduced to the same row echelon form.
\end{theorem}

All this gives us a powerful way to solve linear systems.
The algorithm is described below.

\begin{definition}[Gaussian elimination]
	The process of Gaussian elimination is as follows.
	\begin{enumerate}
		\item Write the augmented matrix of the linear system.
		\item Use elementary row operations to reduce the augmented matrix to row echelon form.
		\item Using back substitution, solve the equivalent system that corresponds to the row-reduced matrix.
	\end{enumerate}
\end{definition}

In many cases, a linear system will have infinitely many solutions.
When this happens, the system's solution set is defined in terms of a certain number of parameters.

\begin{definition}[Leading and free variables]
	The leading variables of a linear system correspond to the leading entries of its augmented matrix.
	The other variables are the free variables.
\end{definition}

\pagebreak

\begin{theorem}[Number of free variables from REF]
	Let $A$ be the coefficient matrix of a linear system with $n$ variables.
	If the system is consistent, then
	\[ \text{\# free variables} = n - (\text{\# nonzero rows in REF}). \]
\end{theorem}

So far, we have been working only with a matrix's row echelon forms.
A certain class of row echelon forms is particularly useful in our study of systems.

\begin{definition}[Reduced row echelon form]
	A matrix is in reduced row echelon form (RREF) if it satisfies the following properties.
	\begin{enumerate}
		\item It is in row echelon form.
		\item The leading entry in each nonzero row is a 1.
		\item Each column containing a leading 1 has zeros everywhere else.
	\end{enumerate}
\end{definition}

One advantage that comes with working with a reduced row echelon form is that it is unique.
Not only that, but reducing a matrix to its RREF gives us an immediate solution to the corresponding linear system.

\begin{definition}[Gauss-Jordan elimination]
	The process of Gauss-Jordan elimination is the same as that of Gaussian elimination, but the matrix is converted into its reduced row echelon form rather than a row echelon form.
\end{definition}

Given either of these simplified forms for a matrix, if the corresponding system takes a certain form, it is easy to tell whether or not a system will have infinitely many solutions.

\begin{definition}[Homogeneous system of linear equations]
	A system of linear equations is homogeneous if the constant term in each equation is zero.
\end{definition}

\begin{theorem}[Solutions of a homogeneous system]
	A homogeneous linear system of $m$ linear equations in $n$ variables has infinitely many solutions if $m < n$.
\end{theorem}

\section{Spanning Sets and Linear Independence}
The study of vectors frequently intersects with that of matrices.
This section lists out some of the introductory definitions and results of this intersection, developing important theory about both objects.
We begin with a condition for the consistency of a linear system.

\begin{theorem}[Condtion for consistency]
	A linear system with augmented matrix $\begin{bmatrix} A \;|\; \mbf{b} \end{bmatrix}$ is consistent if and only if $\mbf{b}$ is a linear combination of the columns of $A$.
\end{theorem}

We'll continue with the idea of linear combinations, using it to define a couple of important concepts.
The first of these gives a name to the vectors that a set of vectors can ``reach'' via a linear combination.

\begin{definition}[Span]
	If $\{\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_k\}$ is a set of vectors in $\R^n$, then the set of all linear combinations of $\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_k$ is called the span of $\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_k$ and is denoted by $\pfn{span}(\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_k)$ or $\pfn{span}(S)$.
	If $\pfn{span}(S) = \R^n$, then the $S$ is called a spanning set for $\R^n$.
\end{definition}

Building off of this, the next definition and the following theorem together define what it means for a vector to be entirely ``separate'' from another set of vectors.

\begin{definition}[Linear dependence]
	A set of vectors $\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_k$ is linearly dependent if there are scalars $c_1, c_2, \ldots, c_k$, at least one of which is not zero, such that
	\[ c_1 \mbf{v}_1 + c_2 \mbf{v}_2 + \cdots + c_k \mbf{v}_k = \mbf{0}. \]
	A set of vectors that is not linearly dependent is called linearly independent.
\end{definition}

\begin{theorem}[Condition for linear dependence]
	Vectors $\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_m$ in $\R^n$ are linearly dependent if and only if at least one of the vectors can be expressed as a linear combination of the others.
\end{theorem}

With this, we can now draw a couple of new connections between linear systems, matrices, and sets of vectors.
The first theorem works with a matrix's column vectors, while the second works with row vectors.

\begin{theorem}[Condition for linearly independent columns]
	Let $\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_m$ be vectors in $\R^n$ and let $A$ be the $n \times m$ matrix with these vectors as its columns.
	Then $\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_m$ are linearly dependent if and only if the homogeneous linear system with augmented matrix $\begin{bmatrix} A \;|\; \mbf{0} \end{bmatrix}$ has a nontrivial solution.
\end{theorem}

\begin{theorem}[Condition for linearly independent rows]
	Let $\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_m$ be vectors in $\R^n$ and let $A$ be the $m \times n$ matrix with these vectors as its rows.
	Then $\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_m$ are linearly dependent if and only if the row echelon form of $A$ has less than $m$ nonzero rows.
\end{theorem}

Finally, we have a powerful sufficient (but not necessary!) condition for linear dependence.

\begin{theorem}[Condition for linear dependence]
	Any set of $m$ vectors in $\R^n$ is linearly dependent if $m > n$.
\end{theorem}

\end{document}