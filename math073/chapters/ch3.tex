\documentclass[../m073main.tex]{subfiles}
\graphicspath{{\subfix{../figures/}}}

\begin{document}

\chapter{Matrices}
\section{Matrix Operations}
Now we turn our attention to matrices in their own right, removed from the context of vectors and linear systems.
They come in several different forms, each of which has its own special properties.

\begin{definition}[Matrix]
	A matrix is a rectangular array of numbers called the entries, or elements, of the matrix.
	A matrix all of whose entries are zero is called the zero matrix $O$.
\end{definition}

\begin{definition}[Special matrices]
	Let $A$ be an $m \times n$ matrix.
	\begin{itemize}
		\item If $ m =n$, then $A$ is called a square matrix.
		\item A square matrix whose nondiagonal entries are all zero is called a diagonal matrix.
		\item A diagonal matrix all of whose diagonal entries are the same is called a scalar matrix.
		\item If the scalar on the diagonal is 1, the scalar matrix is called an identity matrix $I_n$.
	\end{itemize}
\end{definition}

Like vectors, matrices can be added and scaled.

\begin{definition}[Matrix operations]
	Let $A = [a_{ij}]$ and $B_{ij}$ be $m \times n$ matrices, and let $c$ be a scalar.
	Then addition and scalar multiplication are defined componentwise:
	\[ A + B = [a_{ij} + b_{ij}], \quad cA = [ca_{ij}]. \]
\end{definition}

Matrices can also be multiplied with each other, albeit in an at-first unintuitive way.
Suppose we want to multiply $A \mbf{x}$, where $A$ is a matrix and $\mbf{x}$ is a vector; we can think of $A$ as a function that maps one vector to another.
The first column of $A$ tells us what happens to the first component $\mbf{x}$, the second column determines the second component, and so on.
If $\mbf{a}_i$ is the $i$th column of $A$, then the resulting vector is the linear combination
\[ A \mbf{x} = x_1 \mbf{a}_1 + x_2 \mbf{a}_2 + \cdots + x_n \mbf{a}_n. \]
Since a matrix is comprised of several vectors, this provides good motivation for the matrix product.

\begin{definition}[Matrix multiplication]
	If $A$ is an $m\times n$ matrix and $B$ is an $n\times r$ matrix, then the product $C = AB$ is an $m\times r$ matrix where each entry is given by
	\[ c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}. \]
	The matrix-column and row-matrix representations of the product are, respectively,
	\[ AB = \begin{bmatrix} A \mbf{b}_1 & \cdots & A \mbf{b}_r \end{bmatrix} \;\text{ and }\; AB = \begin{bmatrix} \mathbf{A}_1 B \\ \vdots \\ \mathbf{A}_m B \end{bmatrix}. \]
\end{definition}

Sometimes it'll be useful to extract the $i$th row or column from a matrix.
This is easy to accomplish using the left- or right-multiplication of the $i$th standard unit vector, respectively.

We'll define two more operations we can perform on matrices: exponentiation and transposition.

\begin{definition}[Matrix exponentiation and transposition]
	If $A$ is a square matrices, then $A^n = AA\cdots A$, where there are $n$ factors in this product.

	The transpose of an $m\times n$ matrix $A$ is the $n\times m$ matrix $A^T$ obtained by interchanging the rows and columns of $A$.
	That is, the $i$th column of $A^T$ is the $i$th row of $A$ for all $i$, or $(A^T)_{ij} = A_{ji}$ for all $i, j$.
\end{definition}

Visually, the transpose involves flipping a matrix over its main diagonal.
This allows us to define another special, usually non-diagonal type of matrix that exhibits symmetry with respect to transposition.

\begin{definition}[Symmetric matrix]
	A square matrix $A$ is symmetric if $A^T = A$, that is, if $A_{ij} = A_{ji}$ for all $i$ and $j$.
\end{definition}

% Now, using these matrix operations we can define one last operation between vectors.
% It's minor in the context of this course, but still sometimes useful.

% \begin{definition}[Outer product]
% 	Let $\mbf{u}$ and $\mbf{v}$ be vectors in $\R^m$ and $\R^n$, respectively.
% 	Then their outer product is defined as $\mbf{u}\mbf{v}^T$.
% \end{definition}

\section{Matrix Algebra}
The definitions from the previous section lead naturally to some important properties for matrix algebra.
We begin with the basic operations of addition and scalar multiplication, noting the similarities to their corresponding vector operations.

\begin{theorem}[Properties of matrix addition and scalar multiplication]
	Let $A$, $B$, and $C$ be matrices of the same size and let $c$ and $d$ be scalars.
	Then
	\begin{multicols}{2}
		\begin{enumerate}[label=(\alph*)]
			\item $A + B = B + A$
			\item $(A + B) + C = A + (B + C)$
			\item $A + O = A$
			\item $A + (-A) = O$
			\item $c(A + B) = cA + cB$
			\item $(c+d)A = cA + dA$
			\item $c(dA) = (cd)A$
			\item $1A = A$
		\end{enumerate}
	\end{multicols}
\end{theorem}

Now we'll look at matrix multiplication and transposition, noting that multiplication is not commutative!

\begin{theorem}[Properties of matrix multiplication]
	Let $A$, $B$, and $C$ be matrices (such that all indicated operations are defined) and let $k$ be a scalar.
	Then
	\begin{multicols}{2}
		\begin{enumerate}[label=(\alph*)]
			\item $A(BC) = (AB)C$
			\item $A(B+C) = AB+AC$
			\item $(A+B)C = AC+BC$
			\item $k(AB) = (kA)B = A(kB)$
			\item $I_mA = A = AI_n$
		\end{enumerate}
	\end{multicols}
\end{theorem}

\begin{theorem}[Properties of matrix transposition]
	Let $A$ and $B$ be matrices (such that all indicated operations are defined) and let $k$ be a scalar.
	Then
	\begin{multicols}{2}
		\begin{enumerate}[label=(\alph*)]
			\item $(A^T)^T = A$
			\item $(A+B)^T = A^T + B^T$
			\item $(kA)^T = k(A^T)$
			\item $(AB)^T = B^TA^T$
			\item $(A^r)^T = (A^T)^r$
		\end{enumerate}
	\end{multicols}
\end{theorem}

We can use some of these to show what happens when we add or multiply a matrix by its own transpose.

\begin{theorem}[Adding or multiplying by a transpose]
	We have two relationships.
	\begin{enumerate}[label=(\alph*)]
		\item If $A$ is a square matrix, then $A + A^T$ is a symmetric matrix.
		\item For any matrix $A$, $AA^T$ and $A^TA$ are symmetric matrices.
	\end{enumerate}
\end{theorem}

\section{Matrix Inverses}
The additive inverse of a matrix can be found by negating each of its entries, but it might not be surprising that finding a multiplicative inverse is a bit more complicated!
Thankfully, at least one property is familiar.

\begin{definition}[Inverse of a matrix]
	If $A$ is a square matrix, an inverse of $A$ is a matrix $A^{-1}$ of the same size with the property that
	\[ A A^{-1} = I = A^{-1} A. \]
	If such an $A^{-1}$ exists, then $A$ is called invertible.
\end{definition}

\begin{theorem}[Uniqueness of the matirx inverse]
	If $A$ is an invertible matrix, then its inverse is unique.
\end{theorem}

\begin{proof}
	Suppose $A'$ and $A''$ are both inverses of $A$.
	Then
	\[ A' = A'I = A'(AA'') = (A'A)A'' = IA'' = A'', \]
	so $A' = A''$ and the inverse is unique.
\end{proof}

Since we can use matrix inverses to uniquely ``undo'' the effects of matrix multiplication, we can use them to uniquely solve certain equations involving matrices.

\begin{theorem}[Unique solution of a linear system]
	If $A$ is an invertible $n\times n$ matrix, then the system of linear equations given by $A\mbf{x} = \mbf{b}$ has the unique solution $\mbf{x} = A^{-1}\mbf{b}$ for any $\mbf{b}\in\R^n$.
\end{theorem}

In order to apply this to actual systems, we must be able to actually compute the matrix inverse.
Thankfully, the $2 \times 2$ case is relatively straightforward and can be verified by a laborious computation.

\begin{theorem}[Inverse of a two-by-two matrix]
	The matrix $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$ is invertible if $ad - bc \neq 0$, in which case
	\[ A^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}. \]
	If $ad-bc = 0$, then $A$ is not invertible.
\end{theorem}

To get a more general result, we'll need to look at some more easily-verified properties of inverses.

\pagebreak

\begin{theorem}[Properties of matrix inverses]
	If $A$ and $B$ are invertible matrices of the same size, then the following are true:
	\begin{enumerate}[label=(\alph*)]
		\item $A^{-1}$ is invertible and $(A^{-1})^{-1} = A$.
		\item If $c$ is a nonzero scalar, then $cA$ is invertible and $(cA)^{-1} = (1 / c) A^{-1}$.
		\item $AB$ is invertible and $(AB)^{-1} = B^{-1}A^{-1}$.
		\item $A^T$ is invertible and $(A^T)^{-1} = (A^{-1})^T$.
		\item $A^n$ is invertible for all nonnegative integers $n$ and $(A^n)^{-1} = (A^{-1})^n$.
	\end{enumerate}
\end{theorem}

As a side note, property (e) allows for some convenient notation for packaging inverses and exponents together.

\begin{definition}[Negative powers of a matrix]
	If $a$ is an invertible matrix and $n$ is a positive integer, then $A^{-n}$ is defined by
	\[ A^{-n} = (A^{-1})^n = (A^n)^{-1}. \]
\end{definition}

Our upcoming result regarding the inverse of a general square matrix will rely on the formalization of elementary row operations (EROs) using matrix multiplication.
Specifically, each ERO corresponds to a matrix which, when multiplied, is equivalent to performing that ERO.
(Of course, these matrices are invertible since the EROs themselves are easily reversible.)

\begin{definition}[Elementary matrix]
	An elementary matrix is any matrix that can be obtained by performing an elementary row operation on an identity matrix.
\end{definition}

\begin{theorem}[Multiplication by an elementary matrix]
	Let $E$ be the elementary matrix obtained by performing an elementary row operation on $I_n$.
	If the same elementary row operation is performed on an $n\times r$ matrix $A$, the result is the same as the product $EA$.
\end{theorem}

\begin{theorem}[Inverse of an elementary matrix]
	Each elementary matrix is invertible, and its inverse is an elementary matrix of the same type.
\end{theorem}

From here, we can begin formulating a theorem that will not only be useful for our immediate purposes, but which will also reoccur plenty throughout the course: the fundamental theorem of linear algebra (FTLA).
It's essentially a list of useful conditions that are both necessary and sufficient for matrix invertibility.

\begin{theorem}[Additions to the FTLA]
	Let $A$ be an $n\times n$ matrix and let $\mbf{b}$ be a vector in $\R^n$.
	The following statements are equivalent.
	\begin{multicols}{2}
		\begin{enumerate}[label=(\alph*)]
			\item $A$ is invertible.
			\item $A\mbf{x}= \mbf{b}$ has a unique solution.
			\item $A\mbf{x}=\mbf{0}$ has only the trivial solution.
			\item The reduced row echelon form of $A$ is $I_n$.
			\item $A$ is a product of elementary matrices.
		\end{enumerate}
	\end{multicols}
	For the full theorem, see Appendix A.
\end{theorem}

\begin{proof}
	We'll use an implication chain.
	Note that already know (a) $\Rightarrow$ (b), and (c) follows trivially.

	(c) $\Rightarrow$ (d).
	If the system $A \mbf{x} = \mbf{0}$ has the unique solution $\mbf{x} = 0$, then we can reduce the corresponding augmented matrix to find $x_1 = \cdots = x_n = 0$.
	The same reduction $A$ turns into the identity matrix.

	(d) $\Rightarrow$ (e).
	If $A$ reduces to $I_n$, then there are elementary matrices such that $E_k \cdots E_1 A = I_n$.
	Elementary operations are also invertible, so $A = E_k^{-1} \cdots E_1^{-1} I_n = E_k^{-1} \cdots E_1^{-1}$.

	(e) $\Rightarrow$ (a).
	$A$ is a product of elementary matrices, all of which are invertible.
	Thus $A$ is invertible.
\end{proof}

This is a powerful set of results!
We'll use it in a a variety of cases to prove invertibility and, at present, to find a general algorithm for computing matrix inverses.

\begin{theorem}[Condition for matrix invertibility]
	Let $A$ be a square matrix.
	If $B$ is a square matrix such that either $AB = I$ or $BA = I$, then $A$ is invertible and $B = A^{-1}$.
\end{theorem}

\begin{proof}
	Consider the equation $A \mbf{x} = \mbf{0}$, and suppose $BA = I$.
	Then $BA \mbf{x} = B \mbf{0}$ and $I \mbf{x} = \mbf{0}$, meaning $A \mbf{x} = \mbf{0}$ has only the trivial solution and $A$ is invertible.
	The $AB = I$ case is similar, and $B = A^{-1}$ follows from basic algebra.
\end{proof}

\begin{theorem}[Computing the inverse of a matrix]
	Let $A$ be a square matrix.
	If a series of elementary row operations reduces $A$ to $I$, then the same series of elementary row operations transforms $I$ into $A^{-1}$.
\end{theorem}

\begin{proof}
	Suppose there exist elementary matrices such that $(E_k \cdots E_1) A = I$.
	By the previous theorem, the inverse is $A^{-1} = E_k \cdots E_1 = E_k \cdots E_1 I$.
\end{proof}

So in order to invert a matrix, we simply reduce it to the identity matrix while simultaneously performing the same sequence of row operations on the identity.

\section{Subspaces, Basis, Dimension, and Rank}
Now, we'll introduce the ideas of spaces and subspaces, and we'll see how they relate to matrices.

\begin{definition}[Subspace]
	A subspace of $\R^n$ is any collection $S$ of vectors in $\R^n$ such that
	\begin{itemize}
		\item the zero vector $\mbf{0}$ is in $S$ and
		\item $S$ is closed under both addition and scalar multiplication.
	\end{itemize}
\end{definition}

A very simple example of a subspace is the set of all vectors that can be reached by just a few vectors.
Those reachable by the rows or columns of a matrix are particularly relevant.

\begin{theorem}[Span as a subspace]
	Let $S$ be a set of vectors in $\R^n$.
	Then $\pfn{span}(S)$ is a subspace of $\R^n$.
\end{theorem}

\begin{definition}[Row space and column space]
	Let $A$ be an $m\times n$ matrix.
	\begin{itemize}
		\item The row space $\pfn{row}(A)$ of $A$ is the subspace of $\R^n$ spanned by the rows of $A$.
		\item The column space $\pfn{col}(A)$ of $A$ is the subspace of $\R^m$ spanned by the columns of $A$.
	\end{itemize}
\end{definition}

Noting that a matrix's row space is invariant under elementary row operations, we get the following.

\begin{theorem}[Row spaces of row equivalent matrices]
	If $B$ is row equivalent to $A$, then $\pfn{row}(B) = \pfn{row}(A)$.
\end{theorem}

As for the third space of interest, we also note that the solution set of a homogeneous linear system is a subspace.
This can be shown via some simple algebra.

\begin{theorem}[Solution space of a homogeneous linear system]
	Let $A$ be an $m\times n$ matrix and let $S$ be the set of solutions of the homogeneous linear system $A\mbf{x} = \mbf{0}$.
	Then $S$ is a subspace of $\R^n$.
\end{theorem}

\begin{definition}[Null space]
	Let $A$ be an $m\times n$ matrix.
	The null space $\pfn{null}(A)$ of $A$ is the subspace of $A$ consisting of solutions of the homogeneous linear system $A\mbf{x} = \mbf{0}$.
\end{definition}

This notion is interesting: due to the properties of subspaces null spaces must either be empty, singleton, or infinite.
That illuminates something important about systems!

\begin{theorem}[Number of solutions to a linear system]
	Let $A$ be a real-entry matrix.
	The solution set to $A \mbf{x} = \mbf{b}$ is either empty, singleton, or infinite.
\end{theorem}

\begin{proof}
	If (a) or (b) are true, then we are done.
	Otherwise, suppose $\mbf{x}_1$ and $\mbf{x}_2$ are both solutions to the system $A \mbf{x} = \mbf{b}$.
	Their difference $\mbf{x}_0$ solves the associated homogeneous system.
	Now, if the solutions are distinct, then $\mbf{x}_0 \neq \mbf{0}$ and $\pfn{null}(A)$ is infinite.
	Thus $\mbf{x}_1 + c \mbf{x}_0$ solves the system for all $c \in \R$.
\end{proof}

Let's move our discussion along with a new definition.

\begin{definition}[Basis of a subspace]
	A basis for a subspace $S$ of $\R^n$ is a set of vectors in $S$ that
	\begin{itemize}
		\item spans $S$ and
		\item is linearly independent.
	\end{itemize}
\end{definition}

Bases are valuable because they provide the minimum information required to completely describe a subspace.
Thankfully, there is a straightforward way to find a basis for each subspace we've discussed.

\begin{theorem}[Determining bases for row, column, and null spaces]
	Let $R$ be the reduced row echelon form for $A$.
	To find bases for the row, column, and null spaces of a matrix $A$, we do the following.
	\begin{enumerate}[label=(\alph*)]
		\item For $\pfn{row}(A)$, take the nonzero row vectors of $R$.
		\item For $\pfn{col}(A)$, take the column vectors of $A$ that correspond to the pivot columns of $R$.
		\item For $\pfn{null}(A)$, solve the system $A \mbf{x} = R \mbf{x} = \mbf{0}$ and write the solution as a linear combination of vectors times free variables.
	\end{enumerate}
\end{theorem}

No subspace (except for the trivial space $\left\{ \mbf{0} \right\}$) has a unique basis.
However, every basis for a subspace is comprised by the same number of vectors, making it an intrinsic quantity for the subspace.

\pagebreak

\begin{theorem}[Sizes of distinct bases]
	Let $S$ be a subspace of $\R^n$.
	Then any two bases for $S$ have the same number of vectors.
\end{theorem}

\begin{proof}
	Let $\mathcal{B} = \left\{ \mbf{u}_1, \ldots, \mbf{u}_r \right\}$ and $\mathcal{C} = \left\{ \mbf{v}_1, \ldots, \mbf{v}_s \right\}$ be bases for $S$ such that $r < s$, and let
	\[ c_1 \mbf{v}_1 + \cdots + c_s \mbf{v}_s = 0. \]
	Each $\mbf{v}_i$ can be written in terms of the $\mbf{u}_j$; specifically, $\mbf{v}_i = a_{i 1} \mbf{u}_1 + \cdots + a_{i r} \mbf{u}_r$.
	Substituting this into the above and applying the linear independence of $\mathcal{B}$ gives a system of $r$ equations in $c_1, \ldots, c_s$.
	Since there are more variables than equations, this system has infinitely many solutions and the above equation can be satisfied with nontrivial $c_1, \ldots, c_s$, contradicting the linear independence of $\mathcal{C}$.

	Doing the same for $r > s$ yields a similar contradiction, meaning $r = s$.
\end{proof}

\begin{definition}[Dimension]
	If $S$ is a subspace of $\R^n$ then its dimension, denoted $\dim (S)$, is the number of vectors in a basis for $S$.
\end{definition}

It's clear that, for a matrix in row echelon form, the row space has the same dimension as the column space.
But the dimensions of the row and column spaces are invariant under elementary row operations, meaning the dimensions are also the same for any other matrix!

\begin{theorem}[Row and column space dimensions]
	The row and column spaces of a matrix $A$ have the same dimension.
\end{theorem}

\begin{definition}[Rank]
	The rank of a matrix $A$, denoted $\pfn{rank}(A)$, is the common dimension of its row and column spaces.
\end{definition}

Since the row and column spaces of a matrix just get swapped upon transposition, we get the following.

\begin{theorem}[Rank of a transpose]
	For any matrix $A$, $\pfn{rank}(A^T) = \pfn{rank}(A)$.
\end{theorem}

Now, the rank of a matrix is very closely related to its null space.
In terms that we already know, the rank $r$ of a matrix is given by the number of nonzero rows in a row echelon form; its null space has $n-r$ free variables, corresponding to the dimension of the null space.
Adding these together gives the full dimension of the space being acted upon.

\begin{definition}[Nullity]
	The nullity $\pfn{nullity}(A)$ of a matrix $A$ is the dimension of its null space.
\end{definition}

\begin{theorem}[Rank theorem]
	If $A$ is an $m\times n$ matrix, then
	\[ \pfn{rank}(A) + \pfn{nullity}(A) = n. \]
\end{theorem}

We're now in a position to add statements about the row, column, and null spaces to the FTLA.

\pagebreak

\begin{theorem}[Additions to the FTLA]
	Let $A$ be an $n\times n$ matrix.
	The following statements are equivalent.
	\begin{multicols}{2}
		\begin{enumerate}[label=(\alph*)]
			\item $A$ is invertible. \\
			\phantom{~}\hspace{-19.5pt} \vdots
			\setcounter{enumi}{5}		
			\item $\pfn{rank}(A) = n$
			\item $\pfn{nullity}(A) = 0$
			\item The column vectors of $A$ are linearly independent.
			\item The column vectors of $A$ span $\R^n$.
			\item The column vectors of $A$ form a basis for $\R^n$.
			\item The row vectors of $A$ are linearly independent.
			\item The row vectors of $A$ span $\R^n$.
			\item The row vectors of $A$ form a basis for $\R^n$.
		\end{enumerate}
	\end{multicols}
	For the full theorem, see Appendix A.
\end{theorem}

As an application, we have a nice little result that will be useful later.

\begin{theorem}[Rank and invertibility of $A^TA$]
	Let $A$ be an $m\times n$ matrix.
	Then
	\begin{enumerate}[label=(\alph*)]
		\item $\pfn{rank}(A^TA) = \pfn{rank}(A)$ and
		\item $A^TA$ is invertible if and only if $\pfn{rank}(A) = n$.
	\end{enumerate}
\end{theorem}

\begin{proof}
	We only prove (a) since part (b) follows trivially.
	Since $A$ and $A^TA$ both have $n$ columns, it suffices to show that the two matrices have the same null space.

	If $\mbf{x} \in \pfn{null}(A)$ then $A^T A \mbf{x} = A^T \mbf{0} = \mbf{0}$, meaning $\mbf{x} \in \pfn{null} (A^TA)$.
	If instead $\mbf{x} \in \pfn{null} (A^T A)$, then
	\[ (A \mbf{x}) \cdot (A \mbf{x}) = \mbf{x}^T A^T A \mbf{x} = \mbf{x}^T \mbf{0} = \mbf{0}, \]
	so $(A \mbf{x}) \cdot (A \mbf{x}) = \mbf{0}$ and $A \mbf{x} = \mbf{0}$.
	Thus $\mbf{x} \in \pfn{null}(A)$ if and only if $\mbf{x} \in \pfn{null}(A^T A)$.
\end{proof}

There's one more thing that will come in handy later---unique vector representations under a given basis.
At a high level, we'd take two representations $\mbf{u}$ and $\mbf{v}$ and show that each coefficient in $\mbf{u} - \mbf{v}$ must be zero.

\begin{theorem}[Unique representation of a vector]
	Let $S$ be a subspace of $\R^n$ and let $\mathcal{B}$ be a basis for $S$.
	For every vector $\mbf{v}$ in $S$, there is exactly one way to write $\mbf{v}$ as a linear combination of the basis vectors in $\mathcal{B}$.
\end{theorem}

\end{document}