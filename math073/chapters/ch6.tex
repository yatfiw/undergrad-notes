\documentclass[../m073main.tex]{subfiles}
\graphicspath{{\subfix{../figures/}}}

\begin{document}

\chapter{Vector Spaces}
\section{Vector Spaces and Subspaces}
So far we have treated vectors as ordered tuples of real numbers.
However, there are plenty of other objects (like matrices) that behave similarly.
We use the familiar properties of vectors in $\R^n$ to motivate a more general definition for a vector, one that encapsulates this characteristic behavior.

\begin{definition}[Vector space]
	Let $V$ be a set on which two operations, called addition and scalar multiplication, have been defined.
	If $\mbf{u}$ and $\mbf{v}$ are in $V$, the sum of $\mbf{u}$ and $\mbf{v}$ is denoted by $\mbf{u} + \mbf{v}$.
	If $c$ is a scalar, the scalar multiple of $\mbf{u}$ by $c$ is denoted by $c \mbf{u}$.
	If the following axioms hold for all $\mbf{u}, \mbf{v}, \mbf{w} \in V$ and for all scalars $c,d \in \R$, then $V$ is called a vector space and its elements are called vectors.
	\begin{multicols}{2}
		\begin{enumerate}
			\item $\mbf{u} + \mbf{v}$ is in $V$.
			\item $\mbf{u} + \mbf{v} = \mbf{v} + \mbf{u}$.
			\item $(\mbf{u} + \mbf{v}) + \mbf{w} = \mbf{u} + (\mbf{v} + \mbf{w})$.
			\item There exists a $\mbf{0} \in V$ such that $\mbf{u} + \mbf{0} = \mbf{u}$.
			\item For each $\mbf{u}$ in $V$, there is an element $-\mbf{u}$ in $V$ such that $\mbf{u} + (-\mbf{u}) = \mbf{0}$.
			\item $c \mbf{u}$ is in $V$.
			\item $c(\mbf{u} + \mbf{v}) = c \mbf{u} + c \mbf{v}$.
			\item $(c + d) \mbf{u} = c \mbf{u} + d \mbf{u}$.
			\item $c(d \mbf{u}) = (cd) \mbf{u}$.
			\item $1 \mbf{u} = \mbf{u}$.
		\end{enumerate}
	\end{multicols}
	We'll also define the difference between $\mbf{u}$ and $\mbf{v}$, denoted $\mbf{u} - \mbf{v}$, as $\mbf{u} + (-\mbf{v})$.
\end{definition}

This definition allows us to describe many of the ideas we've already seen, just in more general terms.
The rest of this section is dedicated to these, omitting the more straightforward proofs.

\begin{theorem}[Basic properties of vectors]
	Let $V$ be a vector space, $\mbf{u}$ a vector in $V$, and $c$ a scalar.
	\begin{multicols}{2}
		\begin{enumerate}[label=(\alph*)]
			\item $0 \mbf{u} = \mbf{0}$.
			\item $c \mbf{0} = \mbf{0}$.
			\item $(-1) \mbf{0} = -\mbf{u}$.
			\item If $c \mbf{u} = \mbf{0}$, then $c = 0$ or $\mbf{u} = \mbf{0}$.
		\end{enumerate}
	\end{multicols}
\end{theorem}

\begin{proof}
	To prove (b), note that $c \mbf{0} = c (\mbf{0} + \mbf{0}) = c \mbf{0} + c \mbf{0}$.
	Subtracting $c \mbf{0}$ to either side gives $\mbf{0} = c \mbf{0}$.
	As for (d), suppose $c \mbf{u} = \mbf{0}$ and $c \neq 0$; then $\mbf{u} = (1 / c) (c \mbf{u}) = \mbf{0}$.
\end{proof}

\begin{definition}[Linear combination]
	Let $V$ be a vector space, let $\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_n$ be vectors in $V$, and let $c_1, c_2, \ldots, c_n$ be scalars.
	Then $c_1\mbf{v}_1 + c_2\mbf{v}_2 + \cdots + c_n \mbf{v}_n$ is a linear combination of these vectors.
\end{definition}

\begin{definition}[Subspace]
	A subset $W$ of a vector space $V$ is called a subspace of $V$ if $W$ is itself a vector space with the same scalars and operations as $V$.
\end{definition}

The criteria for a subset being a subspace are analogous to the ones we saw previously, and a full justification for them would require showing that the other eight axioms follow.

\begin{theorem}[Conditions for a subspace]
	Let $V$ be a vector space and let $W$ be a nonempty subset of $V$.
	Then $W$ is a subspace of $V$ if and only if the following conditions hold.
	\begin{enumerate}[label=(\alph*)]
		\item If $\mbf{u}$ and $\mbf{v}$ are in $W$, then $\mbf{u} + \mbf{v}$ is in $W$.
		\item If $\mbf{u}$ is in $W$ and $c$ is a scalar, then $c \mbf{u}$ is in $W$.
	\end{enumerate}
\end{theorem}

\begin{definition}[Span]
	If $S = \{\mbf{v}_1, \ldots, \mbf{v}_{k}\}$ is a set of vectors in a vector space $V$, then the set of all linear combinations of $\mbf{v}_1, \ldots, \mbf{v}_k$ is called the span of $\mbf{v}_1, \ldots, \mbf{v}_k$ and is denoted by $\pfn{span}(\mbf{v}_1, \ldots, \mbf{v}_k)$ or $\pfn{span}(S)$.
	If $V = \pfn{span}(S)$, then $S$ is called a spanning set for $V$ and $V$ is said to be spanned by $S$.
\end{definition}

\begin{theorem}[Span as a subspace]
	Let $S$ be a set of vectors in a vector space $V$.
	\begin{enumerate}[label=(\alph*),topsep=0pt]
		\item $\pfn{span}(S)$ is a subspace of $V$.
		\item $\pfn{span}(S)$ is the smallest subspace of $V$ that contains the vectors in $S$.
	\end{enumerate}
\end{theorem}

\begin{proof}
	To prove (b), let $W$ be a subspace of $V$ that has $S$ as a subset.
	$W$ contains every linear combination of the vectors in $S$, meaning $\pfn{span}(S)$ is contained in $W$.
\end{proof}


\section{Linear Independence, Basis, and Dimension}
We'll continue to generalize many of the definitions and properties in $\R^n$ to all vector spaces.
In most cases, the theorems (and their proofs) are identical, just replacing $\R^n$ with $V$.
We begin with linear independence.

\begin{definition}[Linear dependence]
	A set $S$ of vectors in a vector space $V$ is linearly dependent if there is a nontrivial linear combination of the vectors in $S$ that creates the zero vector.
	A set of vectors that is not linearly dependent is said to be linearly independent.
\end{definition}

\begin{theorem}[Condition for linear dependence]
	A set of vectors in a vector space $V$ is linearly dependent if and only if at least one of the vectors can be expressed as a linear combination of the others.
\end{theorem}

With linear independence defined, we can move onto bases.

\begin{definition}[Basis]
	A subset $\mathcal{B}$ of a vector space $V$ is a basis for $V$ if
	\begin{itemize}[topsep=0pt]
		\item $\mathcal{B}$ spans $V$ and
		\item $\mathcal{B}$ is linearly independent.
	\end{itemize}
\end{definition}

\begin{theorem}[Unique representation of a vector]
	Let $V$ be a vector space and let $\mathcal{B}$ be a basis for $V$.
	For every vector $\mbf{v}$ in $V$, there is exactly one way to write $\mbf{v}$ as a linear combination of the basis vectors in $\mathcal{B}$.
\end{theorem}

Now we introduce a new concept, one that we alluded to earlier.
Every basis for a vector space also provides a new coordinate system for that vector space; this coordinate system is defined as follows.

\begin{definition}[Coordinate vector]
	Let $\mathcal{B} = \{\mbf{v}_1, \ldots, \mbf{v}_n\}$ be a basis for a vector space $V$.
	Let $\mbf{v} \in V$, and write $\mbf{v} = c_1 \mbf{v}_1 + \cdots + c_n \mbf{v}_n$.
	Then $c_1, \ldots, c_n$ are called the coordinates of $\mbf{v}$ with respect to $\mathcal{B}$, and the column vector
	\[ [\mbf{v}]_{\mathcal{B}} = \begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix} \]
	is called the coordinate vector of $\mbf{v}$ with respect to $\mathcal{B}$.
\end{definition}

There is a couple of useful things that arise from the use of coordinate systems.
First is a corollary to the previous theorem: coordinate vectors preserve linear combinations.

\begin{theorem}[Coordinates preserve linear combinations]
	Let $\mathcal{B} = \{\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_n\}$ be a basis for a vector space $V$.
	Let $\mbf{u}$ and $\mbf{v}$ be vectors in $V$ and let $c$ be a scalar.
	Then
	\begin{multicols}{2}
		\begin{enumerate}[label=(\alph*)]
			\item $[\mbf{u} + \mbf{v}]_{\mathcal{B}} = [\mbf{u}]_{\mathcal{B}} + [\mbf{v}]_{\mathcal{B}}$ and
			\item $[c \mbf{u}]_{\mathcal{B}} = c[\mbf{u}]_{\mathcal{B}}$.
		\end{enumerate}
	\end{multicols}
	\vspace{6pt}
\end{theorem}

\begin{proof}
	(Sketch) We can write $\mbf{u}$ and $\mbf{v}$ as linear combinations of basis vectors in $V$ and add or multiply them as needed.
\end{proof}

Secondly, coordinates allow us to transfer information from a general vector space to $\R^n$, a space that we have already studied thoroughly; we will explore this idea in detail in the next section.
For now, we can see that a set's linear independence (or dependence) carries over to the set's $\R^n$ counterpart.

\begin{theorem}[Coordinates preserve linear independence]
	Let $\mathcal{B}$ be a basis for a vector space $V$ and let $\mbf{u}_1, \ldots, \mbf{u}_k$ be vectors in $V$.
	Then $\{\mbf{u}_1, \ldots, \mbf{u}_k \}$ is linearly independent in $V$ if and only if $\{[\mbf{u}_1]_{\mathcal{B}}, \, \ldots, \, [\mbf{u}_k]_{\mathcal{B}}\}$ is linearly independent in $\R^n$.
\end{theorem}

\begin{proof}
	(Sketch) We can write a zero linear combination of the coordinate vectors, apply the previous theorem, and use the definition of linear independence to show that the coefficients of the combination are zero.
\end{proof}

Now, having defined basis, we can generalize notions of dimension.
We begin with some motivating theorems, both of which establish the size of a basis as an important quantity.

\begin{theorem}[Necessary conditions for span and linear independence]
	Let $\mathcal{B} = \{\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_n\}$ be a basis for a vector space $V$.
	\begin{enumerate}[label=(\alph*)]
		\item Any set of more than $n$ vectors in $V$ must be linearly dependent.
		\item Any set of fewer than $n$ vectors in $V$ cannot span $V$.
	\end{enumerate}
\end{theorem}

\begin{theorem}[Sizes of distinct bases]
	If a vector space $V$ has a basis with $n$ vectors, then every basis for $V$ has exactly $n$ vectors.
\end{theorem}

Now that we've established that every basis for a vector space contains the same number of vectors, we can define dimension as an intrinsic property of vector spaces.

\begin{definition}[Dimension]
	A vector space $V$ is called finite-dimensional if it has a basis containing finitely many vectors.
	The dimension of $V$, denoted by $\pfn{dim}(V)$, is the number of vectors in a basis for $V$.
	The dimension of the zero vector space $\{\mbf{0}\}$ is defined to be zero.
	A vector space that has no finite basis is called infinite-dimensional.
\end{definition}

Knowing the dimension of a vector space provides us with much information about $V$.
Some of this information is described in the next two theorems.

\begin{theorem}[Dimension, span, and linear independence]
	Let $V$ be a vector space with $\pfn{dim}(V) = n$.
	\begin{enumerate}[label=(\alph*)]
		\item Any linearly independent set in $V$ contains at most $n$ vectors.
		\item Any spanning set for $V$ contains at least $n$ vectors.
		\item Any linearly independent set of exactly $n$ vectors in $V$ is a basis for $V$.
		\item Any spanning set of exactly $n$ vectors in $V$ is a basis for $V$.
		\item Any linearly independent set in $V$ can be extended to a basis for $V$.
		\item Any spanning set for $V$ can be reduced to a basis for $V$.
	\end{enumerate}
\end{theorem}

\begin{theorem}[Dimension of a subspace]
	Let $W$ be a subspace of a finite-dimensional vector space $V$.
	Then:
	\begin{enumerate}[label=(\alph*)]
		\item $W$ is finite-dimensional and $\pfn{dim}(W) \leq \pfn{dim}(V)$.
		\item $\pfn{dim}(W) = \pfn{dim}(V)$ if and only if $W = V$.
	\end{enumerate}
\end{theorem}

\section{Change of Basis}
Equipped with a working knowledge of coordinate systems in general vector spaces, we can now investigate how to convert between coordinate systems within a vector space.
Most of our work here will center around a matrix that does this for us.

\begin{definition}[Change-of-basis matrix]
	Let $\mathcal{B} = \{\mbf{u}_1, \mbf{u}_2, \ldots, \mbf{u}_n\}$ and $\mathcal{C}$ be bases for a vector space $V$.
	The square matrix whose columns are the coordinate vectors $[\mbf{u}_1]_{\mathcal{C}}, [\mbf{u}_2]_{\mathcal{C}}, \ldots, [\mbf{u}_n]_{\mathcal{C}}$ of the vectors in $\mathcal{B}$ with respect to $\mathcal{C}$ is denoted by $P_{\mathcal{C} \from \mathcal{B}}$ and is called the change-of-basis matrix from $\mathcal{B}$ to $\mathcal{C}$.
	That is,
	\[ P_{\mathcal{C} \from \mathcal{B}} = [[\mbf{u}_1]_{\mathcal{C}}, [\mbf{u}_2]_{\mathcal{C}}, \cdots, [\mbf{u}_n]_{\mathcal{C}}]. \]
\end{definition}

If we multiply a vector written with respect to $\mathcal{B}$ by this change-of-basis matrix, we get the same vector written with respect to $\mathcal{C}$.
The change-of-basis matrix has this property and others, as described below.

\begin{theorem}[Properties of the change-of-basis matrix]
	Let $\mathcal{B}$ and $\mathcal{C}$ be bases for a vector space $V$ and let $P_{\mathcal{C} \from \mathcal{B}}$ be the change-of-basis matrix from $\mathcal{B}$ to $\mathcal{C}$.
	\begin{enumerate}[label=(\alph*)]
		\item $P_{\mathcal{C} \from \mathcal{B}}[\mbf{x}]_{\mathcal{B}} = [\mbf{x}]_{\mathcal{C}}$ for all $\mbf{x}$ in $V$.
		\item $P_{\mathcal{C} \from \mathcal{B}}$ is the unique matrix $P$ with the above property.
		\item $P_{\mathcal{C} \from \mathcal{B}}$ is invertible and $(P_{\mathcal{C} \from \mathcal{B}})^{-1} = P_{\mathcal{B} \from \mathcal{C}}$.
	\end{enumerate}
\end{theorem}

\begin{proof}
	For (a) we can simply write
	\[ [\mbf{x}]_\mathcal{C} = [c_1 \mbf{u}_1 + \cdots + c_n \mbf{u}_n]_\mathcal{C} = \begin{bmatrix} [\mbf{u}_1]_\mathcal{C} & \cdots & [\mbf{u}_n]_\mathcal{C} \end{bmatrix} \begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix}. \]
	For (b), let $P$ be a matrix satisfying $P [\mbf{x}]_\mathcal{B}$ for all $\mbf{x}$ in $V$.
	If $\mbf{x} = \mbf{u}_i$ is the $i$th basis vector in $\mathcal B$ then the $i$th column of $P$ is
	\[ \mbf{p}_i = P \mbf{e}_i = P [\mbf{u}_i]_\mathcal{B} = [\mbf{u}_i]_\mathcal{C}. \]
	For (c), notice that the columns of $P_{\mathcal C \from \mathcal B}$ are linearly independent and that $[\mbf{x}]_{\mathcal B} = (P_{\mathcal C \from \mathcal B})^{-1} [\mbf{x}]_{\mathcal C}$.
\end{proof}

There is another way we can compute the change of basis matrix, this time via row reduction.
It is similar to how we compute the inverse of a matrix via row reduction.

\begin{theorem}[Change-of-basis matrix via row reduction]
	Let $\mathcal{B} = \{\mbf{u}_1, \mbf{u}_2, \ldots, \mbf{u}_n\}$ and $\mathcal{C} = \{\mbf{v}_1, \mbf{v}_2, \ldots, \mbf{v}_n\}$ be bases for a vector space $V$.
	Also let $B = [[\mbf{u}_1]_{\mathcal{E}} \cdots [\mbf{u}_n]_{\mathcal{E}}]$ and $C = [[\mbf{v}_1]_{\mathcal{E}} \cdots [\mbf{v}_n]_{\mathcal{E}}]$, where $\mathcal{E}$ is any basis for $V$.
	Then row reduction applied to the $n \times 2n$ augmented matrix $[C \;|\; B]$ produces
	\[ [C \;|\; B] \to [I \;|\; P_{\mathcal{C} \from \mathcal{B}}]. \]
\end{theorem}

\section{Linear Transformations}
Matrices can be interpreted as a transformation between two Euclidean vector spaces.
Here, we extend this concept to linear transformations between arbitrary vector spaces.

\begin{definition}[Linear transformation]
	A linear transformation from a vector space $V$ to a vector space $W$ is a mapping $T : V \to W$ such that, for all $\mbf{u}$ and $\mbf{v}$ in $V$ and for all scalars $c$,
	\begin{multicols}{2}
		\begin{itemize}[topsep=0pt]
			\item $T(\mbf{u} + \mbf{v}) = T(\mbf{u}) + T(\mbf{v})$ and
			\item $T(c \mbf{u}) = cT(\mbf{u})$.
		\end{itemize}
	\end{multicols}
	In other words, $T$ is a linear transformation if and only if it preserves all linear combinations.
\end{definition}

\begin{definition}[Zero and identity transformations]
	The zero transformation $T_0 : V \to W$ maps every vector in $V$ to the zero vector in $W$.
	The identity transformation $I : V \to V$ maps every vector in $V$ to itself.
\end{definition}

Many of the properties of linear transformations are familiar.
Some of these are listed below.

\begin{theorem}[Properties of linear transformations]
	Let $T : V \to W$ be a linear transformation.
	Then:
	\begin{enumerate}[label=(\alph*)]
		\item $T(\mbf{0}) = \mbf{0}$.
		\item $T(-\mbf{v}) = -T(\mbf{v})$ for all $\mbf{v}$ in $V$.
		\item $T(\mbf{u} - \mbf{v}) = T(\mbf{u}) - T(\mbf{v})$ for all $\mbf{u}, \mbf{v}$ in $V$.
	\end{enumerate}
\end{theorem}

The most important property of a linear transformation $T : V \to W$ is that $T$ is completely determined by its effect on a basis for $V$.

\begin{theorem}[Linear transformation using a basis]
	Let $T : V \to W$ be a linear transformation and let $\mathcal{B} = \{\mbf{v}_1, \ldots, \mbf{v}_n\}$ be a spanning set for $V$.
	Then $T(\mathcal{B}) = \{T(\mbf{v}_1), \ldots, T(\mbf{v}_n)\}$ spans the range of $T$.
\end{theorem}

Just like with matrix multiplication, we can compose linear transformations.
The result is a new linear transformation that ``skips'' over the intermediate space.

\begin{definition}[Composition of linear transformations]
	If $T : U \to V$ and $S : V \to W$ are linear transformations, then the composition of $S$ with $T$ is the mapping $S \circ T : U \to W$, defined by $(S \circ T)(\mbf{u}) = S(T(\mbf{u}))$ where $\mbf{u}$ is in $U$.
\end{definition}

\begin{theorem}[Compositions of linear transformations are linear]
	If $T : U \to V$ and $S : V \to W$ are linear transformations, then $S \circ T : U \to W$ is a linear transformation.
\end{theorem}

Lastly, we draw one more generalization from matrices having to do with inverses.

\begin{definition}[Inverse transformation]
	A linear transformation $T : V \to W$ is invertible if there is a linear transformation $T^{-1} : W \to V$ such that
	\[ T^{-1} \circ T = I_V \;\text{ and }\; T \circ T^{-1} = I_W. \]
	In this case, $T^{-1}$ is called an inverse for $T$.
\end{definition}

\begin{theorem}[Condition for invertibility]
	If $T$ is an invertible linear transformation, then its inverse is unique.
\end{theorem}

\section{Kernel and Range}
Now we'll generalize the notions of a matrix's null space and column space.

\begin{definition}[Kernel and range]
	Let $T : V \to W$ be a linear transformation.
	The kernel and range of $T$ are, respectively,
	\[ \pfn{ker}(T) = \{\mbf{v} \in V : T(\mbf{v}) = \mbf{0}\} \;\text{ and }\; \pfn{range}(T) = \{T(\mbf{v}) : \mbf{v} \in V\}. \]
\end{definition}

Like the null and column spaces of a matrix are subspaces of $\R^n$, the kernel and range of a linear transformation are subspaces of the domain and codomain.

\begin{theorem}[Kernel and range as subspaces]
	Let $T : V \to W$ be a linear transformation.
	Then:
	\begin{enumerate}[label=(\alph*)]
		\item $\pfn{ker}(T)$ is a subspace of $V$.
		\item $\pfn{range}(T)$ is a subspace of $W$.
	\end{enumerate}
\end{theorem}

Like before, we assign names to the dimensions of these subspaces and draw a connection between them.

\pagebreak

\begin{definition}[Rank and nullity]
	Let $T : V \to W$ be a linear transformation.
	The rank of $T$ is $\pfn{rank}(T) = \dim (\pfn{range}(T))$ and the nullity of $T$ is $\pfn{nullity}(T) = \dim (\pfn{ker}(T))$.
\end{definition}

\begin{theorem}[Rank theorem]
	Let $T : V \to W$ be a linear transformation where $V$ is finite-dimensional.
	Then
	\[ \pfn{rank}(T) + \pfn{nullity}(T) = \dim V. \]
\end{theorem}

\begin{proof}
	(Sketch) This can be proved using the theory we currently have.
	It's more easily proved, however, in the language of matrices, which we'll develop in the next section.
\end{proof}

Now we have some vocabulary that will help us better describe the input and output spaces of a transformation, along with a couple of criteria which tell us when this vocabulary is valid.

\begin{definition}[One-to-one and onto]
	Let $T : V \to W$ be a linear transformation.
	\begin{itemize}
		\item $T$ is one-to-one if, for all $\mbf{u}, \mbf{v}$ in $V$, $\mbf{u} \neq \mbf{v} \implies T(\mbf{u}) \neq T(v)$.
		\item $T$ is onto if $\pfn{range}(T) = W$.
	\end{itemize}
\end{definition}

\begin{theorem}[Condition for one-to-one]
	A linear transformation $T$ is one-to-one if and only if $\pfn{ker}(T) = \{\mbf{0}\}$
\end{theorem}

\begin{proof}
	The forward direction follows immediately from the definition, so suppose $\ker(T) = \left\{ 0 \right\}$.
	Let $\mbf{u}, \mbf{v} \in V$ satisfy $T(\mbf{u}) = T(\mbf{v})$, so $T(\mbf{u} - \mbf{v}) = \mbf{0}$ and $\mbf{u} - \mbf{v}$ is in the kernel of $T$.
	Thus $\mbf{u} - \mbf{v} = \mbf{0}$ and $\mbf{u} = \mbf{v}$, meaning $T$ is one-to-one.
\end{proof}

\begin{theorem}[Condition for onto]
	Let $\dim V = \dim W$.
	Then a linear transformation $T : V \to W$ is one-to-one if and only if it is onto.
\end{theorem}

\begin{proof}
	By the rank theorem, $\pfn{nullity}(T) = 0$ if and only if $\pfn{rank}(T) = n$, where $\dim W = n$.
\end{proof}

In the previous section, we found that linear transformations, in a sense, preserve spanning sets.
We now provide conditions under which a linear transformation will preserve linear independence and, thus, bases.

\begin{theorem}[One-to-one transformations preserve linear independence]
	Let $T : V \to W$ be a one-to-one linear transformation.
	If $S$ is a linearly independent set in $V$, then $T(S)$ is a linearly independent set in $W$.
\end{theorem}

\begin{corollary}[Condition for the preservation of bases]
	Let $\dim V = \dim W$.
	Then a one-to-one linear transformation $T : V \to W$ maps a basis for $V$ to a basis for $W$.
\end{corollary}

We can use all this to describe which linear transformations are invertible.
(The proof of the following theorem is clunky, but that's just because there're lots of individual things to prove.
Each part is relatively straightforward, so we omit the whole.)

\begin{theorem}[Condition for invertibility]
	A linear transformation $T$ is invertible if and only if it is one-to-one and onto.
\end{theorem}

To finish the section, we concretely define what it means for two vector spaces to be ``essentially the same'' and when, exactly, this happens.

\begin{definition}[Isomorphism]
	A linear transformation $T : V \to W$ is called an isomorphism if it is one-to-one and onto.
	If $V$ and $W$ are two vector spaces such that there is an isomorphism from $V$ to $W$, then we say that $V$ is isomorphic to $W$ and we write $V \cong W$.
\end{definition}
\begin{theorem}[Condition for isomorphism]
	Let $V$ and $W$ be two finite-dimensional vector spaces.
	Then $V$ is isomorphic to $W$ if and only if $\dim V = \dim W$.
\end{theorem}

For our purposes the ``field'' of scalars we're pulling from is $\R$, but really we could build most of the same theory using any field (like $\C$ or $\Z_n$)---see an abstract algebra text for details on what this means.


% ----- TODO: stopped here ----- %

\section{The Matrix of a Linear Transformation}
We can exploit the fact that all $n$-dimensional vector spaces are isomorphic to $\R^n$ to represent linear transformations as matrices.
Rather than transform directly between vector spaces, we will transform between spaces of \textit{coordinate} vectors, as the first theorem of this section shows.

\begin{theorem}[Matrix of a linear transformation]
	Let $V$ and $W$ be two finite-dimensional vector spaces with bases $\mathcal{B}$ and $\mathcal{C}$, respectively, where $\mathcal{B} = \{\mbf{v}_1, \ldots, \mbf{v}_n\}$.
	If $T : V \to W$ is a linear transformation, then the $m \times n$ matrix $A$ defined by
	\[ A = \begin{bmatrix} [T(\mbf{v}_1)]_{\mathcal{C}} & \cdots & [T(\mbf{v}_n)]_{\mathcal{C}} \end{bmatrix} \]
	satisfies
	\[ A [\mbf{v}]_{\mathcal{B}} = [T(\mbf{v})]_{\mathcal{C}} \]
	for every vector $\mbf{v}$ in $V$.
\end{theorem}

This matrix $A$ is called the matrix of $T$ with respect to the bases $\mathcal{B}$ and $\mathcal{C}$; it is sometimes denoted by $[T]_{\mathcal{C} \from \mathcal{B}}$.
We can make some intuitive statements about the matrices of composite and inverse linear transformations.

\begin{theorem}[Composition of matrix transformations]
	Let $U$, $V$, and $W$ be finite-dimensional vector spaces with bases $\mathcal{B}$, $\mathcal{C}$, and $\mathcal{D}$, respectively.
	Let $T : U \to V$ and $S : V \to W$ be linear transformations.
	Then
	\[ [S \circ T]_{\mathcal{D} \from \mathcal{B}} = [S]_{\mathcal{D} \from \mathcal{C}} [T]_{\mathcal{C} \from \mathcal{B}}. \]
\end{theorem}

\begin{theorem}[Inverse of a matrix transformation]
	Let $T : V \to W$ be a linear transformation between $n$-dimensional vector spaces $V$ and $W$ and let $\mathcal{B}$ and $\mathcal{C}$ be bases for $V$ and $W$, respectively.
	Then $T$ is invertible if and only if the matrix $[T]_{\mathcal{C} \from \mathcal{B}}$ is invertible.
	In this case,
	\[ \left( [T]_{\mathcal{C} \from \mathcal{B}} \right)^{-1} = \left[ T^{-1} \right]_{\mathcal{B} \from \mathcal{C}}. \]
\end{theorem}

We can draw a relationship between the potential matrices representing a linear transformation from a vector space to itself.
Namely, we can use the change-of-basis matrix between two bases to show that all of the possible matrices are similar to each other.
This is stated more clearly in the following theorem.

\begin{theorem}[Matrix transformations are similar]
	Let $V$ be a finite-dimensional vector space with bases $\mathcal{B}$ and $\mathcal{C}$ and let $T : V \to V$ be a linear transformation.
	Then
	\[ [T]_{\mathcal{C}} = P^{-1}[T]_{\mathcal{B}}P \]
	where $P$ is the change-of-basis matrix from $\mathcal{C}$ to $\mathcal{B}$.
\end{theorem}

This naturally leads to the notion of diagonalization.

\begin{definition}[Diagonalizible linear transformation]
	Let $V$ be a finite-dimensional vector space and let $T : V \to V$ be a linear transformation.
	Then $T$ is called diagonalizable if there is a basis $\mathcal{C}$ for $V$ such that the matrix $[T]_{\mathcal{C}}$ is a diagonal matrix.
\end{definition}

We can now make some new additions to the Fundamental Theorem.

\begin{theorem}[Additions to the FTLA]
	Let $A$ be an $n\times n$ matrix and let $T : V \to V$ be a linear transformation whose matrix $[T]_{\mathcal{C} \from \mathcal{B}}$ with respect to bases $\mathcal{B}$ and $\mathcal{C}$ of $V$ and $W$, respectively, is $A$.
	The following statements are equivalent.
	\begin{multicols}{2}
		\begin{enumerate}[label=(\alph*)]
			\item $A$ is invertible. \\
			\phantom{~}\hspace{-19.5pt} \vdots
			\setcounter{enumi}{15}
			\item $T$ is invertible.
			\item $T$ is one-to-one.
			\item $T$ is onto.
			\item $\pfn{ker}(T) = \{\mbf{0}\}$.
			\item $\pfn{range}(T) = W$.
			\item[]
		\end{enumerate}
	\end{multicols}
	For the full theorem, see Appendix A.
\end{theorem}

\end{document}